{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imorting libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.pylab as plt\n",
    "import matplotlib.patches as patches\n",
    "import nltk\n",
    "import re\n",
    "from sklearn.metrics import roc_curve,auc\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from bs4 import BeautifulSoup\n",
    "from scipy import interp\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neg</td>\n",
       "      <td>@jamielewislewis i cant believe it, it really ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pos</td>\n",
       "      <td>having a vodka tonic and looking forward to go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pos</td>\n",
       "      <td>@ddlovatofans1neg1 Could you follow me please....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pos</td>\n",
       "      <td>@jordanknight for once.................. PLEAS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>neg</td>\n",
       "      <td>Had a dream about a walk in fast food resturau...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sentiment                                              tweet\n",
       "0       neg  @jamielewislewis i cant believe it, it really ...\n",
       "1       pos  having a vodka tonic and looking forward to go...\n",
       "2       pos  @ddlovatofans1neg1 Could you follow me please....\n",
       "3       pos  @jordanknight for once.................. PLEAS...\n",
       "4       neg  Had a dream about a walk in fast food resturau..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reading data file\n",
    "df =  pd.read_csv( 'sentiment.tsv', header = None, delimiter=\"\\t\")\n",
    "df.columns = ['sentiment', 'tweet']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "          \n",
    "#some spelling correction or converting to meaningful representation. \n",
    "\n",
    "explicit = { \"haven't\": \"have not\",\n",
    "            \"won't\": \"will not\",  \n",
    "            \"i'm\" : \"i am\", \n",
    "            \"didn't\" : \"did not\", \n",
    "            \"don't\" : \"do not\", \n",
    "            \"isn't\" : \"is not\", \n",
    "            \"wasn't\": \"was not\", \n",
    "            \"weren't\" : \"were not\", \n",
    "            \"aren't\" : \"are not\",  \n",
    "            \"couldn't\" : \"could not\",\n",
    "             \"wouldn't\" : \"would not\" , \n",
    "            \"hasn't\" : \"has not\", \n",
    "            \"shalln't\" : \"shall not\", \n",
    "            \"can't\" : \"can not\" , \n",
    "            \"doesn't\" : \"does not\",\n",
    "             \"it's\" : \"it is\",\n",
    "            \"i've\" :\"i have\" ,\n",
    "            \"i'd\" :\"i would\" , \n",
    "            \"he'll\" : \"he will\",\n",
    "            \"she'll\" : \"she will\",\n",
    "            \"you're\" :\"you are\",\n",
    "            \"we're\" :\"we are\",  \n",
    "            \"u're\" : \"you are\",             \n",
    "            \"let's\" : \"let us\" , \n",
    "            \"we'll\" : \"we will\", \n",
    "            \"i'll\" : \"i will\" , \n",
    "            \"you'll\" : \"you will\", \n",
    "            \"that's\" : \"that is\" ,   \n",
    "            \"y'all\" :\"you all\",\n",
    "             \"li'l\" :\"little\",\n",
    "            'ughhh' : 'ugh',\n",
    "            \"suuuupppeeeerrrrr\" : \"super\",    \n",
    "            \"huuurrrttts\" : \"hurts\",\n",
    "            \"grrrrrrrr\" : \"grr\" ,     \n",
    "            \"grrrrrrr\" : \"grr\",          \n",
    "            \"workin\" : \"working\", \n",
    "            \"hear'g\" : \"hearing\", \n",
    "            \"pleanty\" : \"plenty\", \n",
    "            \"suuuupppeeeerrrrr\" : \"super\",     \n",
    "            \"updatessssssssssssssssssssssssssssssssssssssssssssssss\" : \"updates\",\n",
    "           \"u've\" :\"you have\",\n",
    "           \"boredddddddddd\" :\"bored\",\n",
    "           \"daaaaaaang\" :\"dang\",   \n",
    "           \"yeaaaaah\" : \"yeah\",  \n",
    "           \"plllleeeasseee\" :\"please\",\n",
    "           \"yeeeeees\" :\"yes\",\n",
    "           \"yummmmmmmmm\" :\"yummy\",    \n",
    "            \"noooooooooooo\" :\"no\", \n",
    "            \"noooooooooo\" :\"no\",\n",
    "            \"nooooooooo\" :\"no\",    \n",
    "            \"noone\" :\"none\",   \n",
    "            \"nooo\" :\"no\",        \n",
    "            \"hoooooooooooooooooooolla\" :\"hello\",   \n",
    "            \"aaaaaammmazzzingggg\" :\"amazing\",\n",
    "            \"helllllo\" :\"hello\",           \n",
    "            \"loviiiiing\" :\"loving\",\n",
    "            \"xxxxxxxxxx\" : \"xx\",\n",
    "            \"gaaaaaaaaaah\" :\"gah\",\n",
    "            \"gaahhhhh\" :\"gah\", \n",
    "            \"gahh\" :\"gah\",         \n",
    "            \"niceeeeeeeee\" :\"nice\",\n",
    "            \"wellllllll\" :\"well\",      \n",
    "            \"gooooodmorning\" :\"good morning\",\n",
    "            \"loveletter\" : \"love letter\",    \n",
    "            \"betterrrrr\" :\"better\",\n",
    "            \"goooooood\" : \"good\",     \n",
    "            \"wooooow\" :\"wow\",\n",
    "            \"looooove\" :\"love\",\n",
    "            \"luckyyyy\" :\"lucky\", \n",
    "            \"crashiiinnnn\" : \"crashing\",       \n",
    "            \"yearrrr\" : \"year\",\n",
    "            \"overrrr\" : \"over\",   \n",
    "            \"herrrr\" : \"her\",\n",
    "            \"everrr\" : \"ever\" ,     \n",
    "            \"girlllll\" : \"girl\",  \n",
    "            \"alllll\" : \"all\",  \n",
    "            \"alll\" : \"all\",  \n",
    "            \"pleeease\" :\"please\",        \n",
    "            \"historyyyy\" : \"history\", \n",
    "            \"easyyy\" : \"easy\", \n",
    "            \"moreeee\" : \"more\", \n",
    "            \"preetyy\" : \"preety\", \n",
    "            \"lolllll\" : \"lol\",              \n",
    "            \"sweeeet\" : \"sweet\", \n",
    "            \"gr8r\" : \"greater\",         \n",
    "            \"thatsss\" : \"that is\", \n",
    "            \"ggoodd\" : \"good\",\n",
    "            \"omggg\" : \"omg\",\n",
    "            \"destroytwitter\" : \"destroy twitter\", \n",
    "            \"nowww\" :\"now\",      \n",
    "            \"fuckkinq\" :\"fucking\",    \n",
    "            \"worssst\" : \"worst\",\n",
    "            \"lazzzzybum\" : \"lazy bum\",\n",
    "            \"arghhhh\" : \"argh\",   \n",
    "            \"hurtss\" :\"hurts\",  \n",
    "            \"thx\" : \"thanks\",    \n",
    "            \"sickk\" : \"sick\", \n",
    "            \"xxxxxxxxxx\" : \"xx\", \n",
    "            \"knoww\" : \"know\", \n",
    "            \"cryed\" : \"cried\",\n",
    "            \"studiyn\" : \"studying\",\n",
    "            \"badd\" : \"bad\",\n",
    "            \"srry\" : \"sorry\",  \n",
    "            \"ahhhhhhhhhhhhhhh\" : \"ah\",                   \n",
    "            \"ahhhhhhhhhhhh\" : \"ah\", \n",
    "            \"aaahhhhhh\" : \"ah\",           \n",
    "            \"yeahhhh\" : \"yeah\",       \n",
    "            \"ahhhaaa\" : \"ah\",\n",
    "            \"waitiinq\" :\"waiting\",            \n",
    "            \"nighhht\" : \"night\",\n",
    "            \"supportin'\" : \"supporting\",\n",
    "            \"ohwell\" : \"oh well\",  \n",
    "            \"seddd\" : \"said\",\n",
    "            \"thankyou\" : \"thank you\",\n",
    "            \"evenn\" : \"even\",            \n",
    "            \"wayy\" : \"way\",\n",
    "            \"soooooooo\" :\"so\",\n",
    "            \"soooooo\" : \"so\",\n",
    "            \"sooooon\" :\"soon\",\n",
    "            \"soooorry\" :\"sorry\",\n",
    "            \"soooo\" : \"so\",\n",
    "            \"sooo\" :\"so\",         \n",
    "             \"lmaooo\" : \"lmao\", \n",
    "             \"swesome\" : \"awesome\",    \n",
    "            \"doneee\" : \"done\",\n",
    "            \"tummmyyy\" : \"tummy\",\n",
    "            \"quittin'\" : \"quitting\",\n",
    "            \"heeelp\" : \"help\",\n",
    "            \"shortt\" : \"short\",\n",
    "            'yippieee' : 'yippie',\n",
    "            'shaddaaaaap' : 'shut up',\n",
    "            'excitedd' : 'excited',\n",
    "            'iamsoannoyed' : 'i am so annoyed',\n",
    "            'illiegal': 'illegal',\n",
    "            'longg' : 'long',\n",
    "            'ischill' : 'is chill',\n",
    "            'mashallah' : 'God has willed it',\n",
    "            'alhamdulillah' : 'praise be to God',\n",
    "            'saddned' : 'saddened',\n",
    "            'missfabulous' : 'miss fabulous'\n",
    "           }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use these three lines to do the replacement\n",
    "rep = dict((re.escape(k), v) for k, v in explicit.items()) \n",
    "pattern = re.compile(\"|\".join(rep.keys()))\n",
    "\n",
    "stops = set(stopwords.words(\"english\")) #load english words \n",
    "stops -= {'not', 'no', 'none'} #discarding negative terms\n",
    "\n",
    "def review_to_wordlist( review):\n",
    "    review_text = review.lower()\n",
    "    review_text = pattern.sub(lambda m: explicit[re.escape(m.group(0))], review_text)   \n",
    "    review_text = re.sub(r\"\\bshallnt\\b\",  \"shall not\", review_text)\n",
    "    review_text = re.sub(r\"\\bhahah\\b\", \"haha\", review_text)\n",
    "    review_text = re.sub(r\"\\bhahha\\b\", \"haha\", review_text)   \n",
    "    review_text = re.sub(r\"\\bcrampsss\\b\", \"cramps\", review_text)    \n",
    "    review_text = re.sub(r\"\\bboooo\\b\", \"boo\", review_text)\n",
    "    review_text = re.sub(r\"\\bbooo\\b\", \"boo\", review_text)       \n",
    "    review_text = re.sub(r\"\\bwaaaa\\b\", \"wa\", review_text) \n",
    "    review_text = re.sub(r\"\\bgodd\\b\", \"god\", review_text)    \n",
    "    review_text = ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \",review_text).split())\n",
    "    review_text = review_text.strip()\n",
    "    review_text = review_text.split()   \n",
    "    words = [w for w in review_text if not w in stops]    \n",
    "    text = \" \".join(words)\n",
    "    return(text)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tweet</th>\n",
       "      <th>Processed_tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neg</td>\n",
       "      <td>@jamielewislewis i cant believe it, it really ...</td>\n",
       "      <td>cant believe really doesnt belong hope doesnt ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pos</td>\n",
       "      <td>having a vodka tonic and looking forward to go...</td>\n",
       "      <td>vodka tonic looking forward going saddle ranch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pos</td>\n",
       "      <td>@ddlovatofans1neg1 Could you follow me please....</td>\n",
       "      <td>could follow please would really appreciate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pos</td>\n",
       "      <td>@jordanknight for once.................. PLEAS...</td>\n",
       "      <td>please tell us u thinking person</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>neg</td>\n",
       "      <td>Had a dream about a walk in fast food resturau...</td>\n",
       "      <td>dream walk fast food resturaunt sold ice cream...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sentiment                                              tweet  \\\n",
       "0       neg  @jamielewislewis i cant believe it, it really ...   \n",
       "1       pos  having a vodka tonic and looking forward to go...   \n",
       "2       pos  @ddlovatofans1neg1 Could you follow me please....   \n",
       "3       pos  @jordanknight for once.................. PLEAS...   \n",
       "4       neg  Had a dream about a walk in fast food resturau...   \n",
       "\n",
       "                                    Processed_tweets  \n",
       "0  cant believe really doesnt belong hope doesnt ...  \n",
       "1  vodka tonic looking forward going saddle ranch...  \n",
       "2        could follow please would really appreciate  \n",
       "3                   please tell us u thinking person  \n",
       "4  dream walk fast food resturaunt sold ice cream...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tweet.iloc[16] = 'inconvenience is regretted' #16th column is hindi\n",
    "df['Processed_tweets'] = df.tweet.apply(lambda x: review_to_wordlist(x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = dict() \n",
    "   \n",
    "f = open('glove.twitter.27B.200d.txt', encoding=\"utf8\")  \n",
    "\n",
    "#getting vectors as value for words as key in the 'embeddings_index' dictionary\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word =  values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "    \n",
    "f.close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting vectors for few words manually\n",
    "embeddings_index[\"fu**ked\"] = embeddings_index[\"fucked\"]\n",
    "embeddings_index[\"fu**king\"] = embeddings_index[\"fucking\"]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the \"CountVectorizer\" object, which is scikit-learn's bag of words tool.\n",
    "vectorizer = CountVectorizer(analyzer = \"word\", tokenizer = None, preprocessor = None, stop_words = None, max_features = 5000)                             \n",
    "dim = 200 #dimension of the vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import tensorflow\n",
    "from keras.preprocessing.text import Tokenizer, one_hot\n",
    "\n",
    "#Tokenzing all tweets to get the words \n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(df['Processed_tweets'])\n",
    "vocab_size = len(tokenizer.word_index) + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a weight matrix for words in training docs\n",
    "embedding_matrix = np.zeros((vocab_size, dim))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "         embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeFeatureVec(words, vocab, embeddings_index):\n",
    "    # Function to average all of the word vectors in a given\n",
    "    # paragraph\n",
    "    #\n",
    "    # Pre-initialize an empty numpy array (for speed)\n",
    "    featureVec = np.zeros((dim,),dtype=\"float32\")\n",
    "    #\n",
    "    nwords = 0.0\n",
    "    #\n",
    "    # Index2word is a list that contains the names of the words in\n",
    "    # the model's vocabulary. Convert it to a set, for speed\n",
    "    index2word_set = set(vocab)\n",
    "    #\n",
    "    # Loop over each word in the review and, if it is in the model's\n",
    "    # vocaublary, add its feature vector to the total\n",
    "    for word in words:\n",
    "        if word in index2word_set and embeddings_index.get(word) is not None:\n",
    "            nwords = nwords + 1.0\n",
    "            featureVec = np.add(featureVec, embeddings_index.get(word))\n",
    "    #\n",
    "    # Divide the result by the number of words to get the average\n",
    "    #if nwords > 0.0 :\n",
    "    #    featureVec = np.divide(featureVec,nwords)\n",
    "    \n",
    "    return (featureVec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAvgFeatureVecs(reviews, vocab,embeddings_index):\n",
    "    # Given a set of reviews (each one a list of words), calculate\n",
    "    # the average feature vector for each one and return a 2D numpy array\n",
    "    #\n",
    "    # Initialize a counter\n",
    "    counter = 0.\n",
    "    #\n",
    "    # Preallocate a 2D numpy array, for speed\n",
    "    reviewFeatureVecs = np.zeros((len(reviews), dim),dtype=\"float32\")\n",
    "    #\n",
    "    # Loop through the reviews\n",
    "    for review in reviews:\n",
    "        #\n",
    "        # Print a status message every 1000th review\n",
    "        if counter%1000. == 0.:\n",
    "            print (\"Review %d of %d\" % (counter, len(reviews)))\n",
    "       #\n",
    "       # Call the function (defined above) that makes average feature vectors\n",
    "        reviewFeatureVecs[int(counter)] =  makeFeatureVec(review , vocab, embeddings_index)\n",
    "       #\n",
    "       # Increment the counter\n",
    "        counter = counter + 1.\n",
    "        \n",
    "    return (reviewFeatureVecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get a list of words in tweets\n",
    "vocab = []\n",
    "\n",
    "for (k,v) in tokenizer.word_index.items():\n",
    "     vocab.append(k)\n",
    "        \n",
    "vocab1 = set(vocab) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 0 of 2001\n",
      "Review 1000 of 2001\n",
      "Review 2000 of 2001\n"
     ]
    }
   ],
   "source": [
    "def getCleanReviews(reviews):\n",
    "    clean_reviews = []\n",
    "    for review in reviews[\"Processed_tweets\"]:\n",
    "        clean_reviews.append(review.split(\" \"))\n",
    "    return (clean_reviews)\n",
    "\n",
    "trainDataVecs = getAvgFeatureVecs( getCleanReviews(df), vocab, embeddings_index )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n",
      "1800/1800 [==============================] - 1s 700us/step - loss: 0.9146 - accuracy: 0.5717\n",
      "Epoch 2/7\n",
      "1800/1800 [==============================] - 0s 134us/step - loss: 0.6243 - accuracy: 0.6906\n",
      "Epoch 3/7\n",
      "1800/1800 [==============================] - 0s 137us/step - loss: 0.5718 - accuracy: 0.7189\n",
      "Epoch 4/7\n",
      "1800/1800 [==============================] - 0s 134us/step - loss: 0.5179 - accuracy: 0.7494\n",
      "Epoch 5/7\n",
      "1800/1800 [==============================] - 0s 137us/step - loss: 0.4725 - accuracy: 0.7844\n",
      "Epoch 6/7\n",
      "1800/1800 [==============================] - 0s 134us/step - loss: 0.4525 - accuracy: 0.8028\n",
      "Epoch 7/7\n",
      "1800/1800 [==============================] - 0s 129us/step - loss: 0.4117 - accuracy: 0.8211\n",
      "Epoch 1/7\n",
      "1800/1800 [==============================] - 1s 651us/step - loss: 0.7812 - accuracy: 0.5817\n",
      "Epoch 2/7\n",
      "1800/1800 [==============================] - 0s 135us/step - loss: 0.6339 - accuracy: 0.6772\n",
      "Epoch 3/7\n",
      "1800/1800 [==============================] - 0s 142us/step - loss: 0.5713 - accuracy: 0.7106\n",
      "Epoch 4/7\n",
      "1800/1800 [==============================] - 0s 136us/step - loss: 0.5212 - accuracy: 0.7572\n",
      "Epoch 5/7\n",
      "1800/1800 [==============================] - 0s 142us/step - loss: 0.4861 - accuracy: 0.7706\n",
      "Epoch 6/7\n",
      "1800/1800 [==============================] - 0s 143us/step - loss: 0.4390 - accuracy: 0.8039\n",
      "Epoch 7/7\n",
      "1800/1800 [==============================] - 0s 130us/step - loss: 0.4165 - accuracy: 0.8117\n",
      "Epoch 1/7\n",
      "1800/1800 [==============================] - 1s 701us/step - loss: 0.7858 - accuracy: 0.5689\n",
      "Epoch 2/7\n",
      "1800/1800 [==============================] - 0s 142us/step - loss: 0.6173 - accuracy: 0.6767\n",
      "Epoch 3/7\n",
      "1800/1800 [==============================] - 0s 127us/step - loss: 0.5611 - accuracy: 0.7161\n",
      "Epoch 4/7\n",
      "1800/1800 [==============================] - 0s 127us/step - loss: 0.5078 - accuracy: 0.7500\n",
      "Epoch 5/7\n",
      "1800/1800 [==============================] - 0s 143us/step - loss: 0.4909 - accuracy: 0.7683\n",
      "Epoch 6/7\n",
      "1800/1800 [==============================] - 0s 134us/step - loss: 0.4446 - accuracy: 0.7944\n",
      "Epoch 7/7\n",
      "1800/1800 [==============================] - 0s 133us/step - loss: 0.4117 - accuracy: 0.8028\n",
      "Epoch 1/7\n",
      "1800/1800 [==============================] - 1s 713us/step - loss: 0.7346 - accuracy: 0.6067\n",
      "Epoch 2/7\n",
      "1800/1800 [==============================] - 0s 142us/step - loss: 0.5945 - accuracy: 0.6978\n",
      "Epoch 3/7\n",
      "1800/1800 [==============================] - 0s 141us/step - loss: 0.5466 - accuracy: 0.7439\n",
      "Epoch 4/7\n",
      "1800/1800 [==============================] - 0s 139us/step - loss: 0.5272 - accuracy: 0.7456\n",
      "Epoch 5/7\n",
      "1800/1800 [==============================] - 0s 121us/step - loss: 0.4877 - accuracy: 0.7656\n",
      "Epoch 6/7\n",
      "1800/1800 [==============================] - 0s 145us/step - loss: 0.4664 - accuracy: 0.7817\n",
      "Epoch 7/7\n",
      "1800/1800 [==============================] - 0s 147us/step - loss: 0.4448 - accuracy: 0.8039\n",
      "Epoch 1/7\n",
      "1800/1800 [==============================] - 1s 734us/step - loss: 0.7915 - accuracy: 0.6011\n",
      "Epoch 2/7\n",
      "1800/1800 [==============================] - 0s 150us/step - loss: 0.6126 - accuracy: 0.6917\n",
      "Epoch 3/7\n",
      "1800/1800 [==============================] - 0s 145us/step - loss: 0.5581 - accuracy: 0.7239\n",
      "Epoch 4/7\n",
      "1800/1800 [==============================] - 0s 136us/step - loss: 0.5248 - accuracy: 0.7444\n",
      "Epoch 5/7\n",
      "1800/1800 [==============================] - 0s 132us/step - loss: 0.4774 - accuracy: 0.7794\n",
      "Epoch 6/7\n",
      "1800/1800 [==============================] - 0s 133us/step - loss: 0.4530 - accuracy: 0.7939\n",
      "Epoch 7/7\n",
      "1800/1800 [==============================] - 0s 143us/step - loss: 0.4281 - accuracy: 0.8022\n",
      "Epoch 1/7\n",
      "1801/1801 [==============================] - 1s 720us/step - loss: 0.6945 - accuracy: 0.6297\n",
      "Epoch 2/7\n",
      "1801/1801 [==============================] - 0s 203us/step - loss: 0.5776 - accuracy: 0.7163\n",
      "Epoch 3/7\n",
      "1801/1801 [==============================] - 0s 208us/step - loss: 0.5411 - accuracy: 0.7435\n",
      "Epoch 4/7\n",
      "1801/1801 [==============================] - 0s 208us/step - loss: 0.5004 - accuracy: 0.7674\n",
      "Epoch 5/7\n",
      "1801/1801 [==============================] - 0s 203us/step - loss: 0.4581 - accuracy: 0.7896\n",
      "Epoch 6/7\n",
      "1801/1801 [==============================] - 0s 210us/step - loss: 0.4397 - accuracy: 0.7918\n",
      "Epoch 7/7\n",
      "1801/1801 [==============================] - 0s 215us/step - loss: 0.3885 - accuracy: 0.8323\n",
      "Epoch 1/7\n",
      "1802/1802 [==============================] - 1s 737us/step - loss: 0.8495 - accuracy: 0.5766\n",
      "Epoch 2/7\n",
      "1802/1802 [==============================] - 0s 193us/step - loss: 0.6210 - accuracy: 0.6848\n",
      "Epoch 3/7\n",
      "1802/1802 [==============================] - 0s 208us/step - loss: 0.5661 - accuracy: 0.7270\n",
      "Epoch 4/7\n",
      "1802/1802 [==============================] - 0s 220us/step - loss: 0.5206 - accuracy: 0.7553\n",
      "Epoch 5/7\n",
      "1802/1802 [==============================] - 0s 218us/step - loss: 0.5020 - accuracy: 0.7680\n",
      "Epoch 6/7\n",
      "1802/1802 [==============================] - 0s 209us/step - loss: 0.4537 - accuracy: 0.7925\n",
      "Epoch 7/7\n",
      "1802/1802 [==============================] - 0s 218us/step - loss: 0.4196 - accuracy: 0.8130\n",
      "Epoch 1/7\n",
      "1802/1802 [==============================] - 1s 675us/step - loss: 0.7712 - accuracy: 0.6104\n",
      "Epoch 2/7\n",
      "1802/1802 [==============================] - 0s 141us/step - loss: 0.6078 - accuracy: 0.6920\n",
      "Epoch 3/7\n",
      "1802/1802 [==============================] - 0s 148us/step - loss: 0.5542 - accuracy: 0.7242\n",
      "Epoch 4/7\n",
      "1802/1802 [==============================] - 0s 152us/step - loss: 0.5260 - accuracy: 0.7358\n",
      "Epoch 5/7\n",
      "1802/1802 [==============================] - 0s 151us/step - loss: 0.4929 - accuracy: 0.7686\n",
      "Epoch 6/7\n",
      "1802/1802 [==============================] - 0s 135us/step - loss: 0.4429 - accuracy: 0.8024\n",
      "Epoch 7/7\n",
      "1802/1802 [==============================] - 0s 139us/step - loss: 0.4116 - accuracy: 0.8130\n",
      "Epoch 1/7\n",
      "1802/1802 [==============================] - 1s 686us/step - loss: 0.7021 - accuracy: 0.6104\n",
      "Epoch 2/7\n",
      "1802/1802 [==============================] - 0s 140us/step - loss: 0.5919 - accuracy: 0.7109\n",
      "Epoch 3/7\n",
      "1802/1802 [==============================] - 0s 136us/step - loss: 0.5530 - accuracy: 0.7281\n",
      "Epoch 4/7\n",
      "1802/1802 [==============================] - 0s 145us/step - loss: 0.5070 - accuracy: 0.7569\n",
      "Epoch 5/7\n",
      "1802/1802 [==============================] - 0s 149us/step - loss: 0.4776 - accuracy: 0.7880\n",
      "Epoch 6/7\n",
      "1802/1802 [==============================] - 0s 149us/step - loss: 0.4395 - accuracy: 0.7974\n",
      "Epoch 7/7\n",
      "1802/1802 [==============================] - 0s 148us/step - loss: 0.4155 - accuracy: 0.8152\n",
      "Epoch 1/7\n",
      "1802/1802 [==============================] - 1s 737us/step - loss: 0.8587 - accuracy: 0.5782\n",
      "Epoch 2/7\n",
      "1802/1802 [==============================] - 0s 146us/step - loss: 0.6483 - accuracy: 0.6693\n",
      "Epoch 3/7\n",
      "1802/1802 [==============================] - 0s 151us/step - loss: 0.5662 - accuracy: 0.7198\n",
      "Epoch 4/7\n",
      "1802/1802 [==============================] - 0s 150us/step - loss: 0.5361 - accuracy: 0.7270\n",
      "Epoch 5/7\n",
      "1802/1802 [==============================] - 0s 150us/step - loss: 0.4978 - accuracy: 0.7675\n",
      "Epoch 6/7\n",
      "1802/1802 [==============================] - 0s 150us/step - loss: 0.4671 - accuracy: 0.7802\n",
      "Epoch 7/7\n",
      "1802/1802 [==============================] - 0s 145us/step - loss: 0.4387 - accuracy: 0.7919\n"
     ]
    }
   ],
   "source": [
    "cv = StratifiedKFold(n_splits=10,shuffle=False)\n",
    "\n",
    "feeling = {'pos': 1,'neg': 0}\n",
    "df['newfeeling'] = [feeling[item] for item in df.sentiment]\n",
    "\n",
    "\n",
    "tprs = []\n",
    "aucs = []\n",
    "mean_fpr = np.linspace(0,1,100)\n",
    "i = 1\n",
    "x = pd.DataFrame(trainDataVecs)\n",
    "y = df['newfeeling']\n",
    "\n",
    "#model.predict_proba(Xnew)\n",
    "from itertools import chain\n",
    "\n",
    "for train,test in cv.split(x,y):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(80, activation='relu', input_dim=200))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(40, activation='relu'))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "    model.fit(x.iloc[train],y.iloc[train], batch_size= 32, epochs = 7)
    "    prediction = model.predict(x.iloc[test])\n",
    "    lst_prediction = list(chain.from_iterable(prediction))\n",
    "    fpr, tpr, t = roc_curve(y[test], lst_prediction)\n",
    "    tprs.append(interp(mean_fpr, fpr, tpr))\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    aucs.append(roc_auc)\n",
    "    plt.plot(fpr, tpr, lw=2, alpha=0.3, label='ROC fold %d (AUC = %0.2f)' % (i, roc_auc))\n",
    "    i= i+1\n",
    "    \n",
    "print(np.mean(aucs)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
